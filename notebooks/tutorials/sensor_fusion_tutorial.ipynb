{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor Fusion Tutorial\n",
    "\n",
    "This notebook teaches you how to combine data from multiple sensors to create a more robust perception system.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Understanding sensor fusion concepts\n",
    "2. Combining LiDAR and camera data\n",
    "3. Temporal fusion (combining data over time)\n",
    "4. Building a simple fusion algorithm\n",
    "5. Visualizing fused perception data\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of sensors (see [Building Custom Sensors](../building/building_custom_sensors.ipynb))\n",
    "- Understanding of perception data structures\n",
    "- Basic numpy knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_autonomous_car import (\n",
    "    Track,\n",
    "    Car,\n",
    "    CarState,\n",
    "    GroundTruthMap,\n",
    "    PerceivedMap,\n",
    "    LiDARSensor,\n",
    "    PerceptionPoints,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Sensor Fusion\n",
    "\n",
    "Sensor fusion combines data from multiple sensors to:\n",
    "- Reduce uncertainty\n",
    "- Fill gaps in coverage\n",
    "- Improve reliability\n",
    "- Handle sensor failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create setup\n",
    "track = Track.create_simple_track(length=80.0, width=40.0, track_width=5.0)\n",
    "ground_truth_map = GroundTruthMap(track)\n",
    "perceived_map = PerceivedMap(ground_truth_map)\n",
    "\n",
    "start_point, start_heading = track.get_point_at_distance(0.0)\n",
    "car = Car(initial_state=CarState(x=start_point[0], y=start_point[1], heading=start_heading, velocity=8.0))\n",
    "\n",
    "# Add multiple sensors\n",
    "front_lidar = LiDARSensor(\n",
    "    ground_truth_map, perceived_map,\n",
    "    max_range=30.0, angular_resolution=0.1,\n",
    "    name=\"front_lidar\",\n",
    "    pose_ego=np.array([1.0, 0.0, 0.0])  # 1m forward\n",
    ")\n",
    "\n",
    "rear_lidar = LiDARSensor(\n",
    "    ground_truth_map, perceived_map,\n",
    "    max_range=20.0, angular_resolution=0.15,\n",
    "    name=\"rear_lidar\",\n",
    "    pose_ego=np.array([-1.0, 0.0, np.pi])  # 1m back, facing rear\n",
    ")\n",
    "\n",
    "car.add_sensor(front_lidar)\n",
    "car.add_sensor(rear_lidar)\n",
    "\n",
    "print(f\"✓ Setup complete with {len(car.sensors)} sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Simple Fusion - Combine All Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_perception_simple(perception_data: dict, car_state: CarState) -> PerceptionPoints:\n",
    "    \"\"\"\n",
    "    Simple fusion: combine all perception points from all sensors.\n",
    "    \"\"\"\n",
    "    all_points = []\n",
    "    \n",
    "    for sensor_name, points in perception_data.items():\n",
    "        if points is not None and len(points.points) > 0:\n",
    "            # Convert to global frame if needed\n",
    "            if points.frame != \"global\":\n",
    "                points_global = points.to_global_frame(car_state).points\n",
    "            else:\n",
    "                points_global = points.points\n",
    "            \n",
    "            all_points.append(points_global)\n",
    "    \n",
    "    if len(all_points) == 0:\n",
    "        return PerceptionPoints(points=np.array([]).reshape(0, 2), frame=\"global\")\n",
    "    \n",
    "    fused_points = np.vstack(all_points)\n",
    "    return PerceptionPoints(points=fused_points, frame=\"global\")\n",
    "\n",
    "# Test fusion\n",
    "perception_data = car.sense_all(environment_data={\"ground_truth_map\": ground_truth_map})\n",
    "fused = fuse_perception_simple(perception_data, car.state)\n",
    "\n",
    "print(f\"✓ Fused perception: {len(fused.points)} points\")\n",
    "print(f\"  Front LiDAR: {len(perception_data['front_lidar'].points) if perception_data.get('front_lidar') else 0} points\")\n",
    "print(f\"  Rear LiDAR: {len(perception_data['rear_lidar'].points) if perception_data.get('rear_lidar') else 0} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Temporal Fusion - Combine Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalFusion:\n",
    "    \"\"\"\n",
    "    Temporal fusion: combine perception data over multiple time steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_history: int = 5, decay_factor: float = 0.8):\n",
    "        self.max_history = max_history\n",
    "        self.decay_factor = decay_factor\n",
    "        self.history = []  # List of (points, timestamp) tuples\n",
    "    \n",
    "    def fuse(self, current_points: PerceptionPoints, timestamp: float) -> PerceptionPoints:\n",
    "        \"\"\"\n",
    "        Fuse current perception with historical data.\n",
    "        \"\"\"\n",
    "        # Add current points to history\n",
    "        if current_points is not None and len(current_points.points) > 0:\n",
    "            self.history.append((current_points.points.copy(), timestamp))\n",
    "        \n",
    "        # Keep only recent history\n",
    "        if len(self.history) > self.max_history:\n",
    "            self.history.pop(0)\n",
    "        \n",
    "        # Combine all historical points (with decay)\n",
    "        all_points = []\n",
    "        for i, (points, ts) in enumerate(self.history):\n",
    "            age = len(self.history) - 1 - i\n",
    "            weight = self.decay_factor ** age\n",
    "            \n",
    "            # For simplicity, just include all points (could weight by distance)\n",
    "            if weight > 0.3:  # Only include recent enough points\n",
    "                all_points.append(points)\n",
    "        \n",
    "        if len(all_points) == 0:\n",
    "            return PerceptionPoints(points=np.array([]).reshape(0, 2), frame=\"global\")\n",
    "        \n",
    "        fused_points = np.vstack(all_points)\n",
    "        return PerceptionPoints(points=fused_points, frame=\"global\")\n",
    "\n",
    "# Test temporal fusion\n",
    "temporal_fusion = TemporalFusion(max_history=3, decay_factor=0.7)\n",
    "\n",
    "for step in range(5):\n",
    "    perception_data = car.sense_all(environment_data={\"ground_truth_map\": ground_truth_map})\n",
    "    fused_simple = fuse_perception_simple(perception_data, car.state)\n",
    "    fused_temporal = temporal_fusion.fuse(fused_simple, step * 0.1)\n",
    "    \n",
    "    # Move car slightly\n",
    "    car.update(0.1, acceleration=0.5, steering_rate=0.0)\n",
    "    \n",
    "    if step == 4:\n",
    "        print(f\"✓ Temporal fusion after {step+1} steps: {len(fused_temporal.points)} points\")\n",
    "        print(f\"  Current step only: {len(fused_simple.points)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualizing Fused Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_autonomous_car.visualization import plot_track, plot_perception, plot_car\n",
    "\n",
    "# Get current perception\n",
    "perception_data = car.sense_all(environment_data={\"ground_truth_map\": ground_truth_map})\n",
    "fused = fuse_perception_simple(perception_data, car.state)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Left: Individual sensors\n",
    "ax = axes[0]\n",
    "plot_track(track, ax=ax)\n",
    "if perception_data.get('front_lidar'):\n",
    "    plot_perception(perception_data['front_lidar'], car.state, ax=ax, frame=\"global\", color=\"red\", label=\"Front LiDAR\")\n",
    "if perception_data.get('rear_lidar'):\n",
    "    plot_perception(perception_data['rear_lidar'], car.state, ax=ax, frame=\"global\", color=\"orange\", label=\"Rear LiDAR\")\n",
    "plot_car(car, ax=ax, show_heading=True)\n",
    "ax.set_title(\"Individual Sensors\")\n",
    "ax.legend()\n",
    "\n",
    "# Right: Fused data\n",
    "ax = axes[1]\n",
    "plot_track(track, ax=ax)\n",
    "plot_perception(fused, car.state, ax=ax, frame=\"global\", color=\"purple\", label=\"Fused Perception\")\n",
    "plot_car(car, ax=ax, show_heading=True)\n",
    "ax.set_title(\"Fused Perception\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Fusion visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. ✅ **Sensor fusion basics**: Combining data from multiple sensors\n",
    "2. ✅ **Simple fusion**: Merge all perception points\n",
    "3. ✅ **Temporal fusion**: Combine data over time\n",
    "4. ✅ **Visualization**: Compare individual vs fused data\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Spatial fusion**: Combine sensors at same time\n",
    "- **Temporal fusion**: Combine data over time\n",
    "- **Frame conversion**: Ensure all data in same frame\n",
    "- **Weighting**: Can weight sensors by confidence\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Build probabilistic fusion (Kalman filters)\n",
    "- Build confidence-based weighting\n",
    "- Build outlier rejection\n",
    "- Build sensor failure detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

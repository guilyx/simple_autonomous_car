{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Custom Sensors\n",
    "\n",
    "This notebook teaches you how to build custom sensors for the Simple Autonomous Car SDK.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Understanding the sensor architecture\n",
    "2. Building a simple custom sensor\n",
    "3. Building a camera sensor (simulated)\n",
    "4. Building a radar sensor (simulated)\n",
    "5. Combining multiple sensors\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of the SDK basics (see [Building Simulations](building_simulation.ipynb))\n",
    "- Understanding of sensors (see [Building Controller](building_controller.ipynb))\n",
    "- Python knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_autonomous_car import (\n",
    "    Track,\n",
    "    Car,\n",
    "    CarState,\n",
    "    GroundTruthMap,\n",
    "    PerceivedMap,\n",
    "    LiDARSensor,\n",
    "    BaseSensor,\n",
    "    PerceptionPoints,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Sensor Architecture\n",
    "\n",
    "All sensors inherit from `BaseSensor` and must implement:\n",
    "\n",
    "- `sense(car_state, environment_data)`: Returns `PerceptionPoints`\n",
    "- `name`: Sensor identifier\n",
    "- Optional: `pose_ego`: Sensor position/orientation in car frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the BaseSensor interface\n",
    "from simple_autonomous_car.sensors.base_sensor import BaseSensor\n",
    "import inspect\n",
    "\n",
    "print(\"BaseSensor methods:\")\n",
    "for name, method in inspect.getmembers(BaseSensor, predicate=inspect.isfunction):\n",
    "    if not name.startswith('_'):\n",
    "        print(f\"  - {name}{inspect.signature(method)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building a Simple Range Sensor\n",
    "\n",
    "Let's build a simple forward-facing range sensor that detects obstacles ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRangeSensor(BaseSensor):\n",
    "    \"\"\"\n",
    "    A simple forward-facing range sensor.\n",
    "\n",
    "    Detects obstacles in a cone ahead of the sensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ground_truth_map,\n",
    "        perceived_map,\n",
    "        max_range: float = 20.0,\n",
    "        fov_angle: float = np.pi / 6,  # 30 degrees\n",
    "        num_rays: int = 5,\n",
    "        name: str = \"range_sensor\",\n",
    "        pose_ego: np.ndarray = None,\n",
    "    ):\n",
    "        super().__init__(name=name, pose_ego=pose_ego)\n",
    "        self.ground_truth_map = ground_truth_map\n",
    "        self.perceived_map = perceived_map\n",
    "        self.max_range = max_range\n",
    "        self.fov_angle = fov_angle  # Field of view angle\n",
    "        self.num_rays = num_rays\n",
    "\n",
    "    def sense(self, car_state: CarState, environment_data: dict) -> PerceptionPoints:\n",
    "        \"\"\"\n",
    "        Sense obstacles in a forward cone.\n",
    "\n",
    "        Returns points where obstacles are detected.\n",
    "        \"\"\"\n",
    "        # Get sensor pose in global frame\n",
    "        sensor_pose_global = self.get_sensor_pose_global(car_state)\n",
    "        sensor_pos = sensor_pose_global[:2]\n",
    "        sensor_heading = sensor_pose_global[2]\n",
    "\n",
    "        # Generate rays in the field of view\n",
    "        detected_points = []\n",
    "\n",
    "        for i in range(self.num_rays):\n",
    "            # Angle for this ray (spread across FOV)\n",
    "            angle_offset = -self.fov_angle / 2 + (i / (self.num_rays - 1)) * self.fov_angle\n",
    "            ray_angle = sensor_heading + angle_offset\n",
    "\n",
    "            # Cast ray and find intersection with map\n",
    "            ray_end = sensor_pos + self.max_range * np.array([np.cos(ray_angle), np.sin(ray_angle)])\n",
    "\n",
    "            # Simple ray casting: check if ray intersects track boundaries\n",
    "            # In a real implementation, you'd use proper ray casting\n",
    "            intersection = self._ray_cast(sensor_pos, ray_end)\n",
    "\n",
    "            if intersection is not None:\n",
    "                detected_points.append(intersection)\n",
    "\n",
    "        if len(detected_points) == 0:\n",
    "            return PerceptionPoints(points=np.array([]).reshape(0, 2), frame=\"global\")\n",
    "\n",
    "        return PerceptionPoints(points=np.array(detected_points), frame=\"global\")\n",
    "\n",
    "    def _ray_cast(self, start: np.ndarray, end: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simple ray casting to find intersection with track boundaries.\n",
    "\n",
    "        Returns the first intersection point, or None if no intersection.\n",
    "        \"\"\"\n",
    "        # Get track boundaries from ground truth map\n",
    "        track = self.ground_truth_map.track\n",
    "\n",
    "        # Check intersection with inner and outer boundaries\n",
    "        for boundary in [track.inner_bound, track.outer_bound]:\n",
    "            for i in range(len(boundary) - 1):\n",
    "                p1, p2 = boundary[i], boundary[i + 1]\n",
    "\n",
    "                # Line-line intersection\n",
    "                intersection = self._line_intersection(start, end, p1, p2)\n",
    "                if intersection is not None:\n",
    "                    return intersection\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _line_intersection(self, p1, p2, p3, p4):\n",
    "        \"\"\"Find intersection of two line segments.\"\"\"\n",
    "        # Line segment intersection algorithm\n",
    "        d = (p2[0] - p1[0]) * (p4[1] - p3[1]) - (p2[1] - p1[1]) * (p4[0] - p3[0])\n",
    "        if abs(d) < 1e-10:\n",
    "            return None  # Lines are parallel\n",
    "\n",
    "        t = ((p3[0] - p1[0]) * (p4[1] - p3[1]) - (p3[1] - p1[1]) * (p4[0] - p3[0])) / d\n",
    "        u = ((p3[0] - p1[0]) * (p2[1] - p1[1]) - (p3[1] - p1[1]) * (p2[0] - p1[0])) / d\n",
    "\n",
    "        if 0 <= t <= 1 and 0 <= u <= 1:\n",
    "            return p1 + t * (p2 - p1)\n",
    "\n",
    "        return None\n",
    "\n",
    "# Test the sensor\n",
    "track = Track.create_simple_track(length=80.0, width=40.0, track_width=5.0)\n",
    "ground_truth_map = GroundTruthMap(track)\n",
    "perceived_map = PerceivedMap(ground_truth_map)\n",
    "\n",
    "start_point, start_heading = track.get_point_at_distance(0.0)\n",
    "car = Car(initial_state=CarState(x=start_point[0], y=start_point[1], heading=start_heading, velocity=8.0))\n",
    "\n",
    "range_sensor = SimpleRangeSensor(\n",
    "    ground_truth_map,\n",
    "    perceived_map,\n",
    "    max_range=15.0,\n",
    "    fov_angle=np.pi / 4,  # 45 degrees\n",
    "    num_rays=7,\n",
    "    name=\"range_sensor\"\n",
    ")\n",
    "\n",
    "car.add_sensor(range_sensor)\n",
    "\n",
    "# Test sensing\n",
    "perception = range_sensor.sense(car.state, {\"ground_truth_map\": ground_truth_map})\n",
    "\n",
    "print(f\"\u2713 Simple range sensor created!\")\n",
    "print(f\"  Detected {len(perception.points)} points\")\n",
    "print(f\"  Max range: {range_sensor.max_range}m\")\n",
    "print(f\"  FOV: {np.degrees(range_sensor.fov_angle):.1f}\u00b0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualizing the Custom Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_autonomous_car.visualization import plot_track, plot_perception, plot_car\n",
    "\n",
    "# Visualize sensor data\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "plot_track(track, ax=ax)\n",
    "plot_perception(perception, car.state, ax=ax, frame=\"global\", color=\"orange\", label=\"Range Sensor\")\n",
    "plot_car(car, ax=ax, show_heading=True)\n",
    "\n",
    "# Draw sensor FOV\n",
    "sensor_pose = range_sensor.get_sensor_pose_global(car.state)\n",
    "sensor_pos = sensor_pose[:2]\n",
    "sensor_heading = sensor_pose[2]\n",
    "\n",
    "# Draw FOV cone\n",
    "fov_half = range_sensor.fov_angle / 2\n",
    "cone_left = sensor_pos + range_sensor.max_range * np.array([\n",
    "    np.cos(sensor_heading - fov_half),\n",
    "    np.sin(sensor_heading - fov_half)\n",
    "])\n",
    "cone_right = sensor_pos + range_sensor.max_range * np.array([\n",
    "    np.cos(sensor_heading + fov_half),\n",
    "    np.sin(sensor_heading + fov_half)\n",
    "])\n",
    "\n",
    "ax.plot([sensor_pos[0], cone_left[0]], [sensor_pos[1], cone_left[1]], 'g--', alpha=0.5, label='FOV')\n",
    "ax.plot([sensor_pos[0], cone_right[0]], [sensor_pos[1], cone_right[1]], 'g--', alpha=0.5)\n",
    "\n",
    "ax.set_title(\"Simple Range Sensor\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Sensor visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Building a Simulated Camera Sensor\n",
    "\n",
    "A camera sensor detects objects in its field of view and returns bounding boxes or object detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CameraSensor(BaseSensor):\n",
    "    \"\"\"\n",
    "    A simulated camera sensor that detects objects in its field of view.\n",
    "\n",
    "    Returns perception points representing detected objects.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ground_truth_map,\n",
    "        perceived_map,\n",
    "        max_range: float = 30.0,\n",
    "        fov_angle: float = np.pi / 3,  # 60 degrees\n",
    "        resolution: tuple = (640, 480),  # Simulated image resolution\n",
    "        name: str = \"camera\",\n",
    "        pose_ego: np.ndarray = None,\n",
    "    ):\n",
    "        super().__init__(name=name, pose_ego=pose_ego)\n",
    "        self.ground_truth_map = ground_truth_map\n",
    "        self.perceived_map = perceived_map\n",
    "        self.max_range = max_range\n",
    "        self.fov_angle = fov_angle\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def sense(self, car_state: CarState, environment_data: dict) -> PerceptionPoints:\n",
    "        \"\"\"\n",
    "        Simulate camera detection.\n",
    "\n",
    "        In a real implementation, this would process an image.\n",
    "        Here we simulate by detecting track boundaries in FOV.\n",
    "        \"\"\"\n",
    "        # Get sensor pose\n",
    "        sensor_pose_global = self.get_sensor_pose_global(car_state)\n",
    "        sensor_pos = sensor_pose_global[:2]\n",
    "        sensor_heading = sensor_pose_global[2]\n",
    "\n",
    "        # Sample points in FOV (simulating object detection)\n",
    "        detected_points = []\n",
    "        num_samples = 20  # Simulate detecting 20 points\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Random angle in FOV\n",
    "            angle_offset = np.random.uniform(-self.fov_angle / 2, self.fov_angle / 2)\n",
    "            ray_angle = sensor_heading + angle_offset\n",
    "\n",
    "            # Random distance (simulating objects at different distances)\n",
    "            distance = np.random.uniform(5.0, self.max_range)\n",
    "\n",
    "            # Check if this point is on track boundary (simulating object detection)\n",
    "            point = sensor_pos + distance * np.array([np.cos(ray_angle), np.sin(ray_angle)])\n",
    "\n",
    "            # Simple check: if point is near track boundary, it's detected\n",
    "            track = self.ground_truth_map.track\n",
    "            min_dist_to_boundary = min(\n",
    "                np.min(np.linalg.norm(track.inner_bound - point, axis=1)),\n",
    "                np.min(np.linalg.norm(track.outer_bound - point, axis=1))\n",
    "            )\n",
    "\n",
    "            if min_dist_to_boundary < 2.0:  # Within 2m of boundary\n",
    "                detected_points.append(point)\n",
    "\n",
    "        if len(detected_points) == 0:\n",
    "            return PerceptionPoints(points=np.array([]).reshape(0, 2), frame=\"global\")\n",
    "\n",
    "        return PerceptionPoints(points=np.array(detected_points), frame=\"global\")\n",
    "\n",
    "# Test camera sensor\n",
    "camera = CameraSensor(\n",
    "    ground_truth_map,\n",
    "    perceived_map,\n",
    "    max_range=25.0,\n",
    "    fov_angle=np.pi / 2,  # 90 degrees\n",
    "    name=\"camera\"\n",
    ")\n",
    "\n",
    "camera_perception = camera.sense(car.state, {\"ground_truth_map\": ground_truth_map})\n",
    "\n",
    "print(f\"\u2713 Camera sensor created!\")\n",
    "print(f\"  Detected {len(camera_perception.points)} objects\")\n",
    "print(f\"  FOV: {np.degrees(camera.fov_angle):.1f}\u00b0\")\n",
    "print(f\"  Resolution: {camera.resolution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Combining Multiple Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add multiple sensors to the car\n",
    "car = Car(initial_state=CarState(x=start_point[0], y=start_point[1], heading=start_heading, velocity=8.0))\n",
    "\n",
    "# Front LiDAR\n",
    "front_lidar = LiDARSensor(\n",
    "    ground_truth_map,\n",
    "    perceived_map,\n",
    "    max_range=40.0,\n",
    "    name=\"front_lidar\",\n",
    "    pose_ego=np.array([1.0, 0.0, 0.0])  # 1m forward\n",
    ")\n",
    "\n",
    "# Rear range sensor\n",
    "rear_range = SimpleRangeSensor(\n",
    "    ground_truth_map,\n",
    "    perceived_map,\n",
    "    max_range=15.0,\n",
    "    fov_angle=np.pi / 4,\n",
    "    num_rays=5,\n",
    "    name=\"rear_range\",\n",
    "    pose_ego=np.array([-1.0, 0.0, np.pi])  # 1m back, facing rear\n",
    ")\n",
    "\n",
    "# Camera\n",
    "camera = CameraSensor(\n",
    "    ground_truth_map,\n",
    "    perceived_map,\n",
    "    max_range=30.0,\n",
    "    name=\"camera\"\n",
    ")\n",
    "\n",
    "# Add all sensors\n",
    "car.add_sensor(front_lidar)\n",
    "car.add_sensor(rear_range)\n",
    "car.add_sensor(camera)\n",
    "\n",
    "# Get perception from all sensors\n",
    "perception_data = car.sense_all(environment_data={\"ground_truth_map\": ground_truth_map})\n",
    "\n",
    "print(f\"\u2713 Added {len(car.sensors)} sensors to car:\")\n",
    "for sensor in car.sensors:\n",
    "    print(f\"  - {sensor.name}\")\n",
    "\n",
    "print(f\"\\nPerception data from all sensors:\")\n",
    "for sensor_name, points in perception_data.items():\n",
    "    if points is not None:\n",
    "        print(f\"  - {sensor_name}: {len(points.points)} points\")\n",
    "    else:\n",
    "        print(f\"  - {sensor_name}: No data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualizing Multi-Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all sensor data\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "plot_track(track, ax=ax)\n",
    "\n",
    "# Plot each sensor's data with different colors\n",
    "colors = {\"front_lidar\": \"red\", \"rear_range\": \"orange\", \"camera\": \"purple\"}\n",
    "\n",
    "for sensor_name, points in perception_data.items():\n",
    "    if points is not None and len(points.points) > 0:\n",
    "        color = colors.get(sensor_name, \"blue\")\n",
    "        plot_perception(points, car.state, ax=ax, frame=\"global\", color=color, label=sensor_name)\n",
    "\n",
    "plot_car(car, ax=ax, show_heading=True)\n",
    "\n",
    "ax.set_title(\"Multi-Sensor Perception\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Multi-sensor visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. \u2705 **Sensor architecture**: All sensors inherit from `BaseSensor`\n",
    "2. \u2705 **Building custom sensors**: Implement `sense()` method\n",
    "3. \u2705 **Simple range sensor**: Forward-facing obstacle detection\n",
    "4. \u2705 **Camera sensor**: Simulated object detection\n",
    "5. \u2705 **Multi-sensor setup**: Combine multiple sensors on one car\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **BaseSensor**: Base class for all sensors\n",
    "- **PerceptionPoints**: Standard format for sensor data\n",
    "- **Sensor pose**: Position/orientation in car frame\n",
    "- **Modular design**: Easy to add custom sensors\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Build a radar sensor with velocity detection\n",
    "- Build a GPS/IMU sensor for localization\n",
    "- Build sensor fusion algorithms\n",
    "- Add noise models to sensors\n",
    "- Build sensor calibration systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}